---
title: "R Notebook"
output: html_notebook
editor_options: 
  chunk_output_type: console
---
When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Ctrl+Shift+K* to preview the HTML file).

The preview shows you a rendered HTML copy of the contents of the editor. Consequently, unlike *Knit*, *Preview* does not run any R code chunks. Instead, the output of the chunk when it was last run in the editor is displayed.

# <font color='#31708f'>Zendesk Tickets Word Cloud</font>

* [Preprocessing](#second-bullet)

* [WordCloud](#third-bullet)

* [CountVectorizer](#fourth-bullet)

* [LDA Model](#fifth-bullet)

Install package management system 'renv' to ensure packages are installed in the project-specific environment:
```{r}
install.packages("renv")
```

Set your working directory to the project folder.
```{r}
setwd("/home/smsta/Desktop/zendesk_tag_cloud/newman")
```

Activate 'renv' in the project:
```{r}
# Load the renv library
library(renv)

# Initialize renv in the project
renv::init()
```

# <font color='#31708f'>Preprocessing</font>

Display the path of current directory:
```{r}
getwd()
```

```{r}
#Make sure code returns a list of files. List all json files in newman folder, sorted in ascending order
file_list <- list.files(path = "/home/smsta/Desktop/zendesk_tag_cloud/newman", pattern = "*.json")
print(file_list)
```

Install and load the jsonlite package:
```{r}
renv::install("jsonlite")
```

Set your working directory to the project folder.
```{r}
setwd("/home/smsta/Desktop/zendesk_tag_cloud/newman")
```

Read json objects
```{r}
library(jsonlite)

# Specify the file path
file_path <- "/home/smsta/Desktop/zendesk_tag_cloud/newman/newman-run-report-2020-06-25-06-42-13-124-0-ticketid-upto-3000.json"

# Use fromJSON to read the JSON data from the file
x <- fromJSON(file_path)

# Print the first few elements of the resulting object
head(x, 6)
```

Parse json file:
```{r}
library(jsonlite)

file_path <- 'newman-run-report-2020-06-25-06-42-13-124-0-ticketid-upto-3000.json'
json_data <- fromJSON(file_path, flatten = TRUE)

generate_event_tuples <- function(json_data, prefix = NULL) {
  events <- list()

  if (is.list(json_data)) {
    for (key in names(json_data)) {
      current_path <- if (is.null(prefix)) key else paste0(prefix, ".", key)
      value <- json_data[[key]]

      if (is.list(value)) {
        # Recursively call the function for nested objects
        events <- c(events, generate_event_tuples(value, prefix = current_path))
      } else {
        # Add the current key, path, and value to the events list
        events <- c(events, list(c(prefix = current_path, event = key, value = value)))
      }
    }
  } else {
    # Handling named vectors or atomic vectors
    current_path <- if (is.null(prefix)) "root" else prefix
    events <- list(c(prefix = current_path, event = "", value = json_data))
  }

  return(events)
}

result_events <- generate_event_tuples(json_data)

for (event in result_events) {
  cat(sprintf("prefix = %s, event = %s, value = %s\n", event[['prefix']], event[['event']], event[['value']]))
}
```

Parse json files in a folder:
```{r}
library(jsonlite)

for (json_file in file_list) {
  json_data <- fromJSON(json_file)
  
  # Assuming json_data is a list of events
  for (event in json_data) {
    cat(sprintf("prefix = %s, event = %s, value = %s\n", event$prefix, event$event, event$value))
  }
}
```

```{r}
install.packages("tidyjson")
```

```{r}
setwd("/home/smsta/Desktop")
```

```{r}
# Remove escaped newline '\\n' and non-breaking space 'nbsp' characters
  m <- lapply(json_data, function(x) str_replace_all(x, '\\\\n|nbsp', ' '))
  print(m)
```

```{r}
# Load the jsonlite package
library(jsonlite)

# Define a function for streaming JSON parsing
parse_json <- function(json_filename) {
  # Open a connection to the JSON file
  con <- file(json_filename, "r", encoding = "UTF-8")

  # Create a JSON streaming parser
  parser <- stream_in(con)

  # Iterate through the parsed JSON data
  while (length(parser) > 0) {
    # Process each JSON record
    print(parser)

    # Continue parsing the next JSON record
    parser <- stream_in(con, parser)
  }

  # Close the file connection
  close(con)
}

# Specify the JSON file to parse
json_filename <- "newman-run-report-2020-06-25-06-42-13-124-0-ticketid-upto-3000.json"

# Call the parse_json function
parse_json(json_filename)

```

Show all keys in json file:
```{r}
# Load the jsonlite package
library(jsonlite)

# Function to recursively show all keys
show_all_keys <- function(json_data, parent_key = NULL) {
  if (is.list(json_data)) {
    for (key in names(json_data)) {
      nested_key <- ifelse(is.null(parent_key), key, paste(parent_key, key, sep = "."))
      print(nested_key)
      show_all_keys(json_data[[key]], nested_key)
    }
  }
}

# Specify the path to the JSON file
json_file_path <- "/home/smsta/Desktop/zendesk_tag_cloud/newman/newman-run-report-2020-06-25-06-42-13-124-0-ticketid-upto-3000.json"

# Read and parse the JSON file
json_data <- fromJSON(json_file_path)

# Show all keys
print("All Keys:")
show_all_keys(json_data)

```

Access data inside key "assertions":
```{r}
assertions_data <- json_data$run$executions$assertions
assertions_data
```

Extract the substring between two markers:
```{r}
library(stringr)
l <- str_match(assertions_data, "(?<=plain_body)(.+?)(?=public)")
l[1:6]
```

Perform text preprocessing:
```{r}
# Remove escaped newline '\\n' and non-breaking space 'nbsp' characters
m <- gsub("\\\\n|nbsp|\\\\", " ", l, perl = TRUE)
# Remove any URL within a string
p <- gsub("http\\S+|www\\S+", '', m, perl = TRUE)
# Remove all of the punctuation
library(stringi)
q <- gsub("[[:punct:]]", "", p , perl = TRUE)
q[1:6]
```

```{r}
setwd("/home/smsta/Desktop/zendesk_tag_cloud/zendesk_txt")
```


```{r}
install.packages("gtools")
```

```{r}
library(gtools)
```

Order txt files by ticket number:
```{r}
# Get a list of all text files in the "zendesk_txt" directory
all_txt_files <- list.files(path = "/home/smsta/Desktop/zendesk_tag_cloud/zendesk_txt", full.names = TRUE)

# Print the first 6 file names
print(head(all_txt_files, n = 6))

# Count the length of the list
length_all_txt_files <- length(all_txt_files)
print(length_all_txt_files)

# Extract numbers from file names
file_numbers <- as.numeric(gsub("[^0-9]", "", all_txt_files))
file_numbers

# Sort file names based on numbers
sorted_file_names <- all_txt_files[order(file_numbers)]

# Print sorted file names
print(sorted_file_names)

```


```{r}
all_docs <- vector("list", length(all_txt_files))  # Preallocate all_docs list

# Iterate over each text file
for (i in seq_along(all_txt_files)) {
  # Read the content of the text file
  txt_file_as_string <- readLines(all_txt_files[i], encoding = "UTF-8")
  # Store the content in the all_docs list
  all_docs[[i]] <- txt_file_as_string
}

# Combine the first six documents into a single list
first_six_docs <- lapply(all_docs[1:6], paste, collapse = "\n")

# Print the first six documents
print(first_six_docs)


```

```{r}
install.packages("textTinyR")
```

```{r}
library(textTinyR)

# Define a function to tokenize sentences
sent_to_words <- function(sentences) {
  lapply(sentences, function(sentence) {
    tokenize_words(sentence, lowercase = TRUE, remove_punct = TRUE)
  })
}

# Tokenize sentences
data_words <- sent_to_words(all_docs)

# Print first two elements
print(data_words[1:2])
```



```{r}
install.packages(c("tm", "wordcloud"))
```

Generate a word cloud from a directory of text files
```{r}
library(tm)
library(wordcloud)

# Specify the path to the directory containing your text files
directory_path <- "/home/smsta/Desktop/zendesk_tag_cloud/zendesk_txt"

# Create an empty term-document matrix
tdm <- NULL

# Get a list of text files in the specified directory
file_list <- list.files(directory_path, pattern = "\\.txt", full.names = TRUE)

# Create a corpus for all documents
corpus <- Corpus(DirSource(directory_path, encoding = "UTF-8"))

# Preprocess the entire corpus
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removeWords, stopwords("english"))

# Create a sparse term-document matrix for the subset corpus
tdm <- TermDocumentMatrix(corpus, control = list(sparse = TRUE))

# Convert the term-document matrix to a matrix
matrix <- as.matrix(tdm)

# Get the word frequencies
word_freq <- colSums(matrix)

# Create a data frame with words and frequencies
word_df <- data.frame(word = names(word_freq), freq = word_freq)

# Generate a word cloud
wordcloud(words = word_df$word, freq = word_df$freq, min.freq = 1, scale = c(3, 0.5), colors = brewer.pal(8, "Dark2"))

```



